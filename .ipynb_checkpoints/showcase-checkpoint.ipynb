{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a801e4de",
   "metadata": {},
   "source": [
    "# Data Lake Showcase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d86b91",
   "metadata": {},
   "source": [
    "## Environment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c4920c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark.pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2t/5673p3_n6dv9rcn9zx_6f__c0000gn/T/ipykernel_16035/545755688.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark.pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pyspark\n",
    "import pyspark.pandas\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from io import StringIO\n",
    "\n",
    "from hive_metastore_client.builders import DatabaseBuilder\n",
    "from hive_metastore_client import HiveMetastoreClient\n",
    "from thrift_files.libraries.thrift_hive_metastore_client.ttypes import Table, FieldSchema\n",
    "from hive_metastore_client.builders import (\n",
    "    ColumnBuilder,\n",
    "    SerDeInfoBuilder,\n",
    "    StorageDescriptorBuilder,\n",
    "    TableBuilder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab888898",
   "metadata": {},
   "source": [
    "We use a data source in order to utilize Lynx's permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653664c2",
   "metadata": {},
   "source": [
    "## Uploading a test file to the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f35153b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file showcase_data_lake/data/alltypes_dictionary.parquet to visits.csv-08109379/showcase/alltypes_dictionary.parquet\n",
      "File uploaded successfully.\n"
     ]
    }
   ],
   "source": [
    "UPLOADED_FILE_PATH = os.path.join(\"showcase_data_lake\", \"data\", \"alltypes_dictionary.parquet\")\n",
    "SHOW_CASE_BUCKET_DIR_PATH = f\"{data_source.dir}/showcase/\"\n",
    "\n",
    "s3_client = create_s3_client(org_name=data_source.organization_name)\n",
    "s3_target_key = f\"{SHOW_CASE_BUCKET_DIR_PATH}{os.path.basename(UPLOADED_FILE_PATH)}\"\n",
    "\n",
    "print(f\"Uploading file {UPLOADED_FILE_PATH} to {s3_target_key}\")\n",
    "s3_client.upload_file(\n",
    "    Bucket=data_source.bucket,\n",
    "    Key=s3_target_key,\n",
    "    Filename=UPLOADED_FILE_PATH,\n",
    ")\n",
    "print(\"File uploaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4af028a",
   "metadata": {},
   "source": [
    "## Registering the file in Hive metastore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e789fba6",
   "metadata": {},
   "source": [
    "### Create Hive metastore database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6a0e1",
   "metadata": {},
   "source": [
    "Configure Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6e06eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test_Silent_3ef9f5e2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIVE_HOST = \"localhost\"\n",
    "HIVE_PORT = 9083\n",
    "HIVE_METASTORE_DB_NAME = data_source.dataset.programmatic_name.replace(\"-\", \"_\")\n",
    "HIVE_METASTORE_DB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f89051ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "AlreadyExistsException",
     "evalue": "AlreadyExistsException(message='Database Test_Silent_3ef9f5e2 already exists')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAlreadyExistsException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2t/5673p3_n6dv9rcn9zx_6f__c0000gn/T/ipykernel_14089/1719874619.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdatabase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatabaseBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHIVE_METASTORE_DB_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mHiveMetastoreClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHIVE_HOST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIVE_PORT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhive_metastore_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mhive_metastore_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/lynx/lynx-be/venv/lib/python3.7/site-packages/thrift_files/libraries/thrift_hive_metastore_client/ThriftHiveMetastore.py\u001b[0m in \u001b[0;36mcreate_database\u001b[0;34m(self, database)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \"\"\"\n\u001b[1;32m   2071\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_create_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_create_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_create_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/lynx/lynx-be/venv/lib/python3.7/site-packages/thrift_files/libraries/thrift_hive_metastore_client/ThriftHiveMetastore.py\u001b[0m in \u001b[0;36mrecv_create_database\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2092\u001b[0m         \u001b[0miprot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadMessageEnd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2094\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo2\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAlreadyExistsException\u001b[0m: AlreadyExistsException(message='Database Test_Silent_3ef9f5e2 already exists')"
     ]
    }
   ],
   "source": [
    "# Creates a Hive metastore database\n",
    "database = DatabaseBuilder(name=HIVE_METASTORE_DB_NAME).build()\n",
    "with HiveMetastoreClient(HIVE_HOST, HIVE_PORT) as hive_metastore_client:\n",
    "    hive_metastore_client.create_database(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993dfb88",
   "metadata": {},
   "source": [
    "### Create Hive metastore table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec0285e",
   "metadata": {},
   "source": [
    "#### Infer schema columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d6a7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(UPLOADED_FILE_PATH)\n",
    "\n",
    "# [ColumnMetadata(is_primary=True, name='color', primitive_data_type=<PrimitiveDataType.STRING: 'STRING'>),\n",
    "#  ColumnMetadata(is_primary=False, name='value', primitive_data_type=<PrimitiveDataType.STRING: 'STRING'>),\n",
    "#  ColumnMetadata(is_primary=False, name='number', primitive_data_type=<PrimitiveDataType.NUMBER: 'NUMBER'>),\n",
    "#  ColumnMetadata(is_primary=False, name='bool', primitive_data_type=<PrimitiveDataType.BOOLEAN: 'BOOLEAN'>)]\n",
    "inferred_metadata = infer_columns_metadata(df)\n",
    "inferred_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c96cb",
   "metadata": {},
   "source": [
    "#### Create hive columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794daaed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hive_metastore_columns = [\n",
    "#     ColumnBuilder(\"id\", \"string\", \"col comment\").build(),\n",
    "#     ColumnBuilder(\"client_name\", \"string\").build(),\n",
    "]\n",
    "\n",
    "def map_primitive_data_type_to_hive_data_type(primitive_data_type: PrimitiveDataType):\n",
    "    \"\"\"\n",
    "    Hive data types: https://cwiki.apache.org/confluence/display/hive/languagemanual+types\n",
    "    \n",
    "    :return: The Hive datatype\n",
    "    \"\"\"\n",
    "    return {\n",
    "        PrimitiveDataType.STRING.value: \"string\",\n",
    "        PrimitiveDataType.NUMBER.value: \"double\",\n",
    "        PrimitiveDataType.DATETIME.value: \"timestamp\",\n",
    "        PrimitiveDataType.BOOLEAN.value: \"boolean\"\n",
    "    }[primitive_data_type.value]\n",
    "\n",
    "for column in inferred_metadata:\n",
    "    hive_metastore_columns.append(\n",
    "        ColumnBuilder(\n",
    "            column.name,\n",
    "            map_primitive_data_type_to_hive_data_type(column.primitive_data_type),\n",
    "        ).build()\n",
    "    )\n",
    "    \n",
    "hive_metastore_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b36f1",
   "metadata": {},
   "source": [
    "#### Create the Hive metastore table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969442cb",
   "metadata": {},
   "source": [
    "Example: https://github.com/quintoandar/hive-metastore-client/blob/main/examples/create_external_table.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e0253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Underscores are not allowed: https://stackoverflow.com/questions/59631666/create-hive-table-with-hyphen-in-table-name\n",
    "HIVE_TABLE_NAME = data_source.programmatic_name.replace(\"-\", \"_\")\n",
    "HIVE_TABLE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39475201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you table has partitions create a list with the partition columns\n",
    "# This list is similar to the columns list, and the year, month and day\n",
    "# columns are the same.\n",
    "partition_keys = [\n",
    "#     ColumnBuilder(\"year\", \"string\").build(),\n",
    "#     ColumnBuilder(\"month\", \"string\").build(),\n",
    "#     ColumnBuilder(\"day\", \"string\").build(),\n",
    "]\n",
    "\n",
    "serde_info = SerDeInfoBuilder(\n",
    "    serialization_lib=\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"\n",
    ").build()\n",
    "\n",
    "storage_descriptor = StorageDescriptorBuilder(\n",
    "    columns=hive_metastore_columns,\n",
    "    location=f\"s3a://{data_source.bucket}/{SHOW_CASE_BUCKET_DIR_PATH}\",\n",
    "    input_format=\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\",\n",
    "    output_format=\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\",\n",
    "    serde_info=serde_info,\n",
    ").build()\n",
    "\n",
    "table = TableBuilder(\n",
    "    table_name=HIVE_TABLE_NAME,\n",
    "    db_name=HIVE_METASTORE_DB_NAME,\n",
    "    owner=\"owner name\",\n",
    "    storage_descriptor=storage_descriptor,\n",
    "    partition_keys=partition_keys,\n",
    ").build()\n",
    "\n",
    "with HiveMetastoreClient(HIVE_HOST, HIVE_PORT) as hive_metastore_client:\n",
    "    print(f\"Creating table {HIVE_TABLE_NAME} in database {HIVE_METASTORE_DB_NAME}\")\n",
    "    # Creating new table from thrift table object\n",
    "    hive_metastore_client.create_external_table(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b53fa",
   "metadata": {},
   "source": [
    "## Query the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c136ad9",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c956447",
   "metadata": {},
   "source": [
    "Docs:\n",
    "- https://spark.apache.org/docs/latest/configuration.html\n",
    "- https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8332b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "spark = (\n",
    "    SparkSession \n",
    "        .builder\n",
    "        .master(\"spark://localhost:7077\")\n",
    "        .appName(\"SparkHiveMetastoreTest\")\n",
    "        .config(\"spark.sql.uris\", \"thrift://localhost:9083\")\n",
    "        .config(\"hive.metastore.warehouse.dir\", \"thrift://localhost:9083\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5803e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca95fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
